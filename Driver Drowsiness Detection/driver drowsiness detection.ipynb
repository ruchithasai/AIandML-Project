{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Drive Link : [https://drive.google.com/file/d/18o_9SXqe-XgcYFwlkse5CIe132G5TyxH/view?usp=sharing]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: parameters in c:\\anaconda\\lib\\site-packages (0.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pygame in c:\\anaconda\\lib\\site-packages (1.9.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "from parameters import *\n",
    "from scipy.spatial import distance\n",
    "from imutils import face_utils as face\n",
    "from pygame import mixer\n",
    "import imutils\n",
    "import time\n",
    "import dlib\n",
    "import cv2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "shape_predictor_path    = os.path.join('data','C:\\\\Users\\\\Ruchi\\\\Desktop\\\\driver\\\\shape_predictor_68_face_landmarks (1).dat')\n",
    "alarm_paths             = [os.path.join('data','audio_files','C:\\\\Users\\\\Ruchi\\\\Desktop\\\\driver\\\\short_horn.wav'),\n",
    "                           os.path.join('data','audio_files','C:\\\\Users\\\\Ruchi\\\\Desktop\\\\driver\\\\long_horn.wav'),\n",
    "                           os.path.join('data','audio_files','C:\\\\Users\\\\Ruchi\\\\Desktop\\\\driver\\\\distraction_alert.wav')]\n",
    "\n",
    "# defining some values for the reference\n",
    "\n",
    "EYE_DROWSINESS_THRESHOLD    = 0.25\n",
    "EYE_DROWSINESS_INTERVAL     = 2.0\n",
    "MOUTH_DROWSINESS_THRESHOLD  = 0.37\n",
    "MOUTH_DROWSINESS_INTERVAL   = 1.0\n",
    "DISTRACTION_INTERVAL        = 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some supporting functions for facial processing\n",
    "\n",
    "def get_max_area_rect(rects):\n",
    "    if len(rects)==0: return\n",
    "    areas=[]\n",
    "    for rect in rects:\n",
    "        areas.append(rect.area())\n",
    "    return rects[areas.index(max(areas))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the euclidean distances between the two sets of vertical eye and horizantal eye\n",
    "\n",
    "def get_eye_aspect_ratio(eye):\n",
    "    vertical_1 = distance.euclidean(eye[1], eye[5])\n",
    "    vertical_2 = distance.euclidean(eye[2], eye[4])\n",
    "    horizontal = distance.euclidean(eye[0], eye[3])\n",
    "    return (vertical_1+vertical_2)/(horizontal*2) # compute the eye aspect ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the euclidean distances between the mouth corners\n",
    "\n",
    "def get_mouth_aspect_ratio(mouth):\n",
    "    horizontal=distance.euclidean(mouth[0],mouth[4])\n",
    "    vertical=0\n",
    "    for coord in range(1,4):\n",
    "        vertical+=distance.euclidean(mouth[coord],mouth[8-coord])\n",
    "    return vertical/(horizontal*3) # compute the mouth aspect ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facial processing\n",
    "\n",
    "def facial_processing():\n",
    "    mixer.init()\n",
    "    distracton_initlized = False\n",
    "    eye_initialized      = False\n",
    "    mouth_initialized    = False\n",
    "    \n",
    "# initialize dlib's face detector\n",
    "# creating the facial landmark predictor    \n",
    "\n",
    "    detector    = dlib.get_frontal_face_detector()\n",
    "    predictor   = dlib.shape_predictor(shape_predictor_path)\n",
    "    \n",
    "# grabing the indexes of the facial landmarks for the left and right eye by using FACIAL_LANDMARKS_IDXS  \n",
    "\n",
    "    ls,le = face.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "    rs,re = face.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
    "\n",
    "# to record a video from the webcam   \n",
    "\n",
    "    cap=cv2.VideoCapture(0)\n",
    "    \n",
    "# loop over frames from the video stream\n",
    "# grab the frame from the threaded video resize it, and convert it to grayscale\n",
    "\n",
    "    fps_couter=0\n",
    "    fps_to_display='initializing...'\n",
    "    fps_timer=time.time()\n",
    "    while True:\n",
    "        _ , frame=cap.read()\n",
    "        fps_couter+=1\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        if time.time()-fps_timer>=1.0:\n",
    "            fps_to_display=fps_couter\n",
    "            fps_timer=time.time()\n",
    "            fps_couter=0\n",
    "        cv2.putText(frame, \"FPS :\"+str(fps_to_display), (frame.shape[1]-100, frame.shape[0]-10),\\\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "\n",
    "        #frame = imutils.resize(frame, width=900)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# detect faces in the grayscale frame\n",
    "        \n",
    "        rects = detector(gray, 0)\n",
    "        rect=get_max_area_rect(rects)\n",
    "        \n",
    "# loop over the face detections\n",
    "\n",
    "        if rect!=None:\n",
    "\n",
    "            distracton_initlized=False\n",
    "\n",
    "            shape = predictor(gray, rect)\n",
    "            shape = face.shape_to_np(shape)\n",
    "            \n",
    "            # extracting the left and right eye coordinates, then using the\n",
    "            # coordinates to compute the eye aspect ratio for both eyes\n",
    "\n",
    "            leftEye = shape[ls:le]\n",
    "            rightEye = shape[rs:re]\n",
    "            leftEAR = get_eye_aspect_ratio(leftEye)\n",
    "            rightEAR = get_eye_aspect_ratio(rightEye)\n",
    "\n",
    "            inner_lips=shape[60:68]\n",
    "            mar=get_mouth_aspect_ratio(inner_lips)\n",
    "\n",
    "            eye_aspect_ratio = (leftEAR + rightEAR) / 2.0  # average the eye aspect ratio together for both eyes\n",
    "\n",
    "            # compute the convex hull for the left and right eye, then visualize each of the eyes\n",
    "            \n",
    "            leftEyeHull = cv2.convexHull(leftEye)\n",
    "            rightEyeHull = cv2.convexHull(rightEye)\n",
    "            cv2.drawContours(frame, [leftEyeHull], -1, (255, 255, 255), 1)\n",
    "            cv2.drawContours(frame, [rightEyeHull], -1, (255, 255, 255), 1)\n",
    "            lipHull = cv2.convexHull(inner_lips)\n",
    "            cv2.drawContours(frame, [lipHull], -1, (255, 255, 255), 1)\n",
    "\n",
    "            cv2.putText(frame, \"EAR: {:.2f} MAR{:.2f}\".format(eye_aspect_ratio,mar), (10, frame.shape[0]-10),\\\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "# check if the eye aspect ratio is below the EYE_DROWSINESS_THRESHOLD            \n",
    "            \n",
    "            if eye_aspect_ratio < EYE_DROWSINESS_THRESHOLD:\n",
    "\n",
    "                if not eye_initialized:\n",
    "                    eye_start_time= time.time()\n",
    "                    eye_initialized=True\n",
    "                    \n",
    "#if the eyes were closed for a sufficient time then sound the alarm\n",
    "    \n",
    "                if time.time()-eye_start_time >= EYE_DROWSINESS_INTERVAL:\n",
    "                    alarm_type=0\n",
    "                    cv2.putText(frame, \"YOU ARE SLEEPY...\\nPLEASE TAKE A BREAK!\", (10, 20),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "\n",
    "                    if  not distracton_initlized and not mouth_initialized and not mixer.music.get_busy():\n",
    "                        mixer.music.load(alarm_paths[alarm_type])\n",
    "                        mixer.music.play()\n",
    "            else:\n",
    "                eye_initialized=False\n",
    "                if not distracton_initlized and not mouth_initialized and mixer.music.get_busy():\n",
    "                    mixer.music.stop()\n",
    "\n",
    "# check if the mar is below the MOUTH_DROWSINESS_THRESHOLD                    \n",
    "\n",
    "            if mar > MOUTH_DROWSINESS_THRESHOLD:\n",
    "\n",
    "                if not mouth_initialized:\n",
    "                    mouth_start_time= time.time()\n",
    "                    mouth_initialized=True\n",
    "\n",
    "# if the mouth was opened for a sufficient time then sound the alarm                   \n",
    "                    \n",
    "                if time.time()-mouth_start_time >= MOUTH_DROWSINESS_INTERVAL:\n",
    "                    alarm_type=0\n",
    "                    cv2.putText(frame, \"YOU ARE YAWNING...\\nDO YOU NEED A BREAK?\", (10, 40),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "\n",
    "                    if not mixer.music.get_busy():\n",
    "                        mixer.music.load(alarm_paths[alarm_type])\n",
    "                        mixer.music.play()\n",
    "            else:\n",
    "                mouth_initialized=False\n",
    "                if not distracton_initlized and not eye_initialized and mixer.music.get_busy():\n",
    "                    mixer.music.stop()\n",
    "\n",
    "\n",
    "                    \n",
    "        else:\n",
    "            alarm_type=1\n",
    "            if not distracton_initlized:\n",
    "                distracton_start_time=time.time()\n",
    "                distracton_initlized=True\n",
    "\n",
    "            if time.time()- distracton_start_time> DISTRACTION_INTERVAL:\n",
    "\n",
    "                cv2.putText(frame, \"EYES ON ROAD\", (10, 20),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "\n",
    "                if not eye_initialized and not mouth_initialized and not  mixer.music.get_busy():\n",
    "                    mixer.music.load(alarm_paths[alarm_type])\n",
    "                    mixer.music.play()\n",
    "        # show the frame       \n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        key = cv2.waitKey(5)&0xFF\n",
    "        if key == ord(\"q\"):    # if the `q` key was pressed, break from the loop\n",
    "            break\n",
    "# cleanup\n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release()\n",
    "\n",
    "if __name__=='__main__':\n",
    "    facial_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
